{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Sample Factory \u00b6 Codebase for high throughput asynchronous reinforcement learning. Paper: https://arxiv.org/abs/2006.11751 Cite: BibTeX Talk: https://youtu.be/lLG17LKKSZc Videos: https://sites.google.com/view/sample-factory VizDoom agents trained with Sample Factory playing in real time: When should I use Sample Factory? \u00b6 Sample Factory is the fastest open source single-machine RL implementations (see paper for details). If you plan to train RL agents on large amounts of experience, consider using it. Sample Factory can significantly speed up the experimentation or allow you to collect more samples in the same amount of time and achieve better performance. Consider using Sample Factory for your multi-agent and population-based training experiments. Multi-agent and PBT setups are really simple with Sample Factory. A lot of work went into our VizDoom and DMLab wrappers. For example, we include full support for configurable VizDoom multi-agent environments and their interop with RL algorithms, which can open new interesting research directions. Consider using Sample Factory if you train agents in these environments. Sample Factory can be a good choice as a prototype for a single node in a distributed RL system or as a reference codebase for other types of async RL algorithms.","title":"Overview"},{"location":"#sample-factory","text":"Codebase for high throughput asynchronous reinforcement learning. Paper: https://arxiv.org/abs/2006.11751 Cite: BibTeX Talk: https://youtu.be/lLG17LKKSZc Videos: https://sites.google.com/view/sample-factory VizDoom agents trained with Sample Factory playing in real time:","title":"Sample Factory"},{"location":"#when-should-i-use-sample-factory","text":"Sample Factory is the fastest open source single-machine RL implementations (see paper for details). If you plan to train RL agents on large amounts of experience, consider using it. Sample Factory can significantly speed up the experimentation or allow you to collect more samples in the same amount of time and achieve better performance. Consider using Sample Factory for your multi-agent and population-based training experiments. Multi-agent and PBT setups are really simple with Sample Factory. A lot of work went into our VizDoom and DMLab wrappers. For example, we include full support for configurable VizDoom multi-agent environments and their interop with RL algorithms, which can open new interesting research directions. Consider using Sample Factory if you train agents in these environments. Sample Factory can be a good choice as a prototype for a single node in a distributed RL system or as a reference codebase for other types of async RL algorithms.","title":"When should I use Sample Factory?"},{"location":"community/citation/","text":"Citation \u00b6 If you use this repository in your work or otherwise wish to cite it, please make reference to our ICML2020 paper. @inproceedings{petrenko2020sf, title={Sample Factory: Egocentric 3D Control from Pixels at 100000 FPS with Asynchronous Reinforcement Learning}, author={Petrenko, Aleksei and Huang, Zhehui and Kumar, Tushar and Sukhatme, Gaurav and Koltun, Vladlen}, booktitle={ICML}, year={2020} } For questions, issues, inquiries please email apetrenko1991@gmail.com . Github issues and pull requests are welcome.","title":"Citation"},{"location":"community/citation/#citation","text":"If you use this repository in your work or otherwise wish to cite it, please make reference to our ICML2020 paper. @inproceedings{petrenko2020sf, title={Sample Factory: Egocentric 3D Control from Pixels at 100000 FPS with Asynchronous Reinforcement Learning}, author={Petrenko, Aleksei and Huang, Zhehui and Kumar, Tushar and Sukhatme, Gaurav and Koltun, Vladlen}, booktitle={ICML}, year={2020} } For questions, issues, inquiries please email apetrenko1991@gmail.com . Github issues and pull requests are welcome.","title":"Citation"},{"location":"community/contribution/","text":"Contribute to SF \u00b6 How to contribute to Sample Factory? \u00b6 Sample Factory is an open source project, so all contributions and suggestions are welcome. You can contribute in many different ways: giving ideas, answering questions, reporting bugs, proposing enhancements, improving the documentation, fixing bugs,... Many thanks in advance to every contributor. How to work on an open Issue? \u00b6 You have the list of open Issues at: https://github.com/alex-petrenko/sample-factory/issues Some of them may have the label help wanted : that means that any contributor is welcomed! If you would like to work on any of the open Issues: Make sure it is not already assigned to someone else. You have the assignee (if any) on the top of the right column of the Issue page. You can self-assign it by commenting on the Issue page with one of the keywords: #take or #self-assign . Work on your self-assigned issue and eventually create a Pull Request. How to create a Pull Request? \u00b6 Fork the repository by clicking on the 'Fork' button on the repository's page. This creates a copy of the code under your GitHub user account. Clone your fork to your local disk, and add the base repository as a remote: git clone git@github.com:<your Github handle>/sample-factory.git cd sample-factory git remote add upstream https://github.com/alex-petrenko/sample-factory.git Create a new branch to hold your development changes: git checkout -b a-descriptive-name-for-my-changes do not work on the main branch. Set up a development environment by running the following command in a virtual (or conda) environment: pip install -e . [ dev ] (If sample-factory was already installed in the virtual environment, remove it with pip uninstall sample-factory before reinstalling it in editable mode with the -e flag.) Develop the features on your branch. Format your code. Run black and isort so that your newly added files look nice with the following command: make format 7. Run unittests with the following command: make test 8. Once you're happy with your files, add your changes and make a commit to record your changes locally: git add sample-factory/<your_dataset_name> git commit It is a good idea to sync your copy of the code with the original repository regularly. This way you can quickly account for changes: git fetch upstream git rebase upstream/main Push the changes to your account using: git push -u origin a-descriptive-name-for-my-changes Once you are satisfied, go the webpage of your fork on GitHub. Click on \"Pull request\" to send your to the project maintainers for review.","title":"Contribute to SF"},{"location":"community/contribution/#contribute-to-sf","text":"","title":"Contribute to SF"},{"location":"community/contribution/#how-to-contribute-to-sample-factory","text":"Sample Factory is an open source project, so all contributions and suggestions are welcome. You can contribute in many different ways: giving ideas, answering questions, reporting bugs, proposing enhancements, improving the documentation, fixing bugs,... Many thanks in advance to every contributor.","title":"How to contribute to Sample Factory?"},{"location":"community/contribution/#how-to-work-on-an-open-issue","text":"You have the list of open Issues at: https://github.com/alex-petrenko/sample-factory/issues Some of them may have the label help wanted : that means that any contributor is welcomed! If you would like to work on any of the open Issues: Make sure it is not already assigned to someone else. You have the assignee (if any) on the top of the right column of the Issue page. You can self-assign it by commenting on the Issue page with one of the keywords: #take or #self-assign . Work on your self-assigned issue and eventually create a Pull Request.","title":"How to work on an open Issue?"},{"location":"community/contribution/#how-to-create-a-pull-request","text":"Fork the repository by clicking on the 'Fork' button on the repository's page. This creates a copy of the code under your GitHub user account. Clone your fork to your local disk, and add the base repository as a remote: git clone git@github.com:<your Github handle>/sample-factory.git cd sample-factory git remote add upstream https://github.com/alex-petrenko/sample-factory.git Create a new branch to hold your development changes: git checkout -b a-descriptive-name-for-my-changes do not work on the main branch. Set up a development environment by running the following command in a virtual (or conda) environment: pip install -e . [ dev ] (If sample-factory was already installed in the virtual environment, remove it with pip uninstall sample-factory before reinstalling it in editable mode with the -e flag.) Develop the features on your branch. Format your code. Run black and isort so that your newly added files look nice with the following command: make format 7. Run unittests with the following command: make test 8. Once you're happy with your files, add your changes and make a commit to record your changes locally: git add sample-factory/<your_dataset_name> git commit It is a good idea to sync your copy of the code with the original repository regularly. This way you can quickly account for changes: git fetch upstream git rebase upstream/main Push the changes to your account using: git push -u origin a-descriptive-name-for-my-changes Once you are satisfied, go the webpage of your fork on GitHub. Click on \"Pull request\" to send your to the project maintainers for review.","title":"How to create a Pull Request?"},{"location":"community/doc-contribution/","text":"Doc Contribution \u00b6 workflows \u00b6 clone the target repo It should contain a \u2018docs\u2019 folder, a \u2018mkdocs.yml\u2019 config file, a \u2018docs.yml\u2019 github actions file. install common dependencies pip install mkdocs-material pip install mkdocs-minify-plugin pip install mkdocs-redirects pip install mkdocs-git-revision-date-localized-plugin pip install mkdocs-git-committers-plugin-2 pip install mkdocs-git-authors-plugin serve the website locally mkdocs serve you should see the website on your localhost port now. modify or create markdown files modify / create your markdown files in \u2018docs\u2019 folder. add your markdown path in the \u2018nav\u2019 section of \u2018mkdocs.yml\u2019(at the bottom). Example folder-yml correspondence: commit and push your changes to remote repo. github actions will automatically push your changes to your github pages website.","title":"Doc Contribution"},{"location":"community/doc-contribution/#doc-contribution","text":"","title":"Doc Contribution"},{"location":"community/doc-contribution/#workflows","text":"clone the target repo It should contain a \u2018docs\u2019 folder, a \u2018mkdocs.yml\u2019 config file, a \u2018docs.yml\u2019 github actions file. install common dependencies pip install mkdocs-material pip install mkdocs-minify-plugin pip install mkdocs-redirects pip install mkdocs-git-revision-date-localized-plugin pip install mkdocs-git-committers-plugin-2 pip install mkdocs-git-authors-plugin serve the website locally mkdocs serve you should see the website on your localhost port now. modify or create markdown files modify / create your markdown files in \u2018docs\u2019 folder. add your markdown path in the \u2018nav\u2019 section of \u2018mkdocs.yml\u2019(at the bottom). Example folder-yml correspondence: commit and push your changes to remote repo. github actions will automatically push your changes to your github pages website.","title":"workflows"},{"location":"environment-integrations/atari/","text":"Atari Integrations \u00b6 Installation \u00b6 Install Sample-Factory with Atari dependencies with PyPI: pip install sample-factory[atari] Running Experiments \u00b6 Run Atari experiments with the scripts in sf_examples.atari_examples . The default parameters have been chosen to match CleanRL's configuration (see reports below) and are not tuned for throughput. TODO: provide parameters that result in faster training. To train a model in the BreakoutNoFrameskip-v4 environment: python -m sf_examples.atari_examples.train_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\" To visualize the training results, use the enjoy_atari script: python -m sf_examples.atari_examples.enjoy_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\" Multiple experiments can be run in parallel with the runner module. atari_envs is an example runner script that runs atari envs with 4 seeds. python -m sample_factory.runner.run --run=sf_examples.atari_examples.experiments.atari_envs --runner=processes --max_parallel=8 --pause_between=1 --experiments_per_gpu=10000 --num_gpus=1 List of Supported Environments \u00b6 Specify the environment to run with the --env command line parameter. The following Atari v4 environments are supported out of the box, and more enviornments can be added as needed in sf_examples.atari_examples.atari.atari_utils Atari Environment Name Atari Command Line Parameter BreakoutNoFrameskip-v4 atari_breakout PongNoFrameskip-v4 atari_pong BeamRiderNoFrameskip-v4 atari_beamrider Results \u00b6 Reports \u00b6 Sample-Factory was benchmarked on Atari against CleanRL and Baselines. Sample-Factory was able to achieve similar sample efficiency as CleanRL and Baselines using the same parameters. https://wandb.ai/wmfrank/atari-benchmark/reports/Atari-Sample-Factory2-Baselines-CleanRL--VmlldzoyMzEyNjIw Models \u00b6 Various APPO models trained on Atari environments are uploaded to the HuggingFace Hub. The models have all been trained for 10M steps. Videos of the agents after training can be found on the HuggingFace Hub. Environment HuggingFace Hub Models Evaluation Metrics BreakoutNoFrameskip-v4 https://huggingface.co/wmFrank/sample-factory-2-atari-breakout 30.20 \u00b1 23.45 PongNoFrameskip-v4 https://huggingface.co/wmFrank/sample-factory-2-atari-pong 13.50 \u00b1 7.43 BeamRiderNoFrameskip-v4 https://huggingface.co/wmFrank/sample-factory-2-atari-beamrider 3848.00 \u00b1 308.00 Videos \u00b6 Below are some video examples of agents in various Atari envioronments. Videos for all enviornments can be found in the HuggingFace Hub pages linked above. BreakoutNoFrameskip-v4 \u00b6 PongNoFrameskip-v4 \u00b6 BeamRiderNoFrameskip-v4 \u00b6","title":"Atari Integrations"},{"location":"environment-integrations/atari/#atari-integrations","text":"","title":"Atari Integrations"},{"location":"environment-integrations/atari/#installation","text":"Install Sample-Factory with Atari dependencies with PyPI: pip install sample-factory[atari]","title":"Installation"},{"location":"environment-integrations/atari/#running-experiments","text":"Run Atari experiments with the scripts in sf_examples.atari_examples . The default parameters have been chosen to match CleanRL's configuration (see reports below) and are not tuned for throughput. TODO: provide parameters that result in faster training. To train a model in the BreakoutNoFrameskip-v4 environment: python -m sf_examples.atari_examples.train_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\" To visualize the training results, use the enjoy_atari script: python -m sf_examples.atari_examples.enjoy_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\" Multiple experiments can be run in parallel with the runner module. atari_envs is an example runner script that runs atari envs with 4 seeds. python -m sample_factory.runner.run --run=sf_examples.atari_examples.experiments.atari_envs --runner=processes --max_parallel=8 --pause_between=1 --experiments_per_gpu=10000 --num_gpus=1","title":"Running Experiments"},{"location":"environment-integrations/atari/#list-of-supported-environments","text":"Specify the environment to run with the --env command line parameter. The following Atari v4 environments are supported out of the box, and more enviornments can be added as needed in sf_examples.atari_examples.atari.atari_utils Atari Environment Name Atari Command Line Parameter BreakoutNoFrameskip-v4 atari_breakout PongNoFrameskip-v4 atari_pong BeamRiderNoFrameskip-v4 atari_beamrider","title":"List of Supported Environments"},{"location":"environment-integrations/atari/#results","text":"","title":"Results"},{"location":"environment-integrations/atari/#reports","text":"Sample-Factory was benchmarked on Atari against CleanRL and Baselines. Sample-Factory was able to achieve similar sample efficiency as CleanRL and Baselines using the same parameters. https://wandb.ai/wmfrank/atari-benchmark/reports/Atari-Sample-Factory2-Baselines-CleanRL--VmlldzoyMzEyNjIw","title":"Reports"},{"location":"environment-integrations/atari/#models","text":"Various APPO models trained on Atari environments are uploaded to the HuggingFace Hub. The models have all been trained for 10M steps. Videos of the agents after training can be found on the HuggingFace Hub. Environment HuggingFace Hub Models Evaluation Metrics BreakoutNoFrameskip-v4 https://huggingface.co/wmFrank/sample-factory-2-atari-breakout 30.20 \u00b1 23.45 PongNoFrameskip-v4 https://huggingface.co/wmFrank/sample-factory-2-atari-pong 13.50 \u00b1 7.43 BeamRiderNoFrameskip-v4 https://huggingface.co/wmFrank/sample-factory-2-atari-beamrider 3848.00 \u00b1 308.00","title":"Models"},{"location":"environment-integrations/atari/#videos","text":"Below are some video examples of agents in various Atari envioronments. Videos for all enviornments can be found in the HuggingFace Hub pages linked above.","title":"Videos"},{"location":"environment-integrations/atari/#breakoutnoframeskip-v4","text":"","title":"BreakoutNoFrameskip-v4"},{"location":"environment-integrations/atari/#pongnoframeskip-v4","text":"","title":"PongNoFrameskip-v4"},{"location":"environment-integrations/atari/#beamridernoframeskip-v4","text":"","title":"BeamRiderNoFrameskip-v4"},{"location":"environment-integrations/megaverse/","text":"Megaverse \u00b6 Megaverse is a dedicated high-throughput RL environment with batched GPU rendering. This document demonstrates an example of external integration, i.e. another project using Sample Factory as a library. Very likely this is going to be the most common integration scenario. Installation \u00b6 Install Megaverse according to the readme of the repo Megaverse . Further instructions assume that you are in a Python (or Conda) environment with a working Megaverse installation. Running Experiments \u00b6 Run Megaverse experiments with the scripts in megaverse_rl . To train a model in the TowerBuilding environment: python -m megaverse_rl.train_megaverse --train_for_seconds=360000000 --train_for_env_steps=2000000000 --algo=APPO --gamma=0.997 --use_rnn=True --rnn_num_layers=2 --num_workers=12 --num_envs_per_worker=2 --num_epochs=1 --rollout=32 --recurrence=32 --batch_size=4096 --actor_worker_gpus 0 --env_gpu_observations=False --num_policies=1 --with_pbt=False --max_grad_norm=0.0 --exploration_loss=symmetric_kl --exploration_loss_coeff=0.001 --megaverse_num_simulation_threads=1 --megaverse_num_envs_per_instance=32 --megaverse_num_agents_per_env=1 --megaverse_use_vulkan=True --policy_workers_per_policy=2 --reward_clip=30 --env=TowerBuilding --experiment=TowerBuilding To visualize the training results, use the enjoy_megaverse script: python -m megaverse_rl.enjoy_megaverse --algo=APPO --env=TowerBuilding --experiment=TowerBuilding --megaverse_num_envs_per_instance=1 --fps=20 --megaverse_use_vulkan=True Multiple experiments can be run in parallel with the runner module. megaverse_envs is an example runner script that runs atari envs with 4 seeds. python -m sample_factory.runner.run --run=megaverse_rl.runs.single_agent --runner=processes --max_parallel=2 --pause_between=1 --experiments_per_gpu=2 --num_gpus=1 Or you could run experiments on slurm: python -m sample_factory.runner.run --run=megaverse_rl.runs.single_agent --runner=slurm --slurm_workdir=./slurm_megaverse --experiment_suffix=slurm --slurm_gpus_per_job=1 --slurm_cpus_per_gpu=16 --slurm_sbatch_template=./sample_factory/runner/slurm/sbatch_template.sh --pause_between=1 --slurm_print_only=False Results \u00b6 Reports \u00b6 We trained models in the TowerBuilding environment in SF2 with single agent per env. https://wandb.ai/wmfrank/megaverse-benchmark/reports/Megaverse-trained-Sample-Factory--VmlldzoyNTAxMDUz Models \u00b6 An example APPO model trained on Megaverse environments is uploaded to the HuggingFace Hub. The models have all been trained for 2G steps. Environment HuggingFace Hub Models TowerBuilding https://huggingface.co/wmFrank/sample-factory-2-megaverse Videos \u00b6 Tower Building with single agent \u00b6 Tower Building with four agents \u00b6","title":"Megaverse"},{"location":"environment-integrations/megaverse/#megaverse","text":"Megaverse is a dedicated high-throughput RL environment with batched GPU rendering. This document demonstrates an example of external integration, i.e. another project using Sample Factory as a library. Very likely this is going to be the most common integration scenario.","title":"Megaverse"},{"location":"environment-integrations/megaverse/#installation","text":"Install Megaverse according to the readme of the repo Megaverse . Further instructions assume that you are in a Python (or Conda) environment with a working Megaverse installation.","title":"Installation"},{"location":"environment-integrations/megaverse/#running-experiments","text":"Run Megaverse experiments with the scripts in megaverse_rl . To train a model in the TowerBuilding environment: python -m megaverse_rl.train_megaverse --train_for_seconds=360000000 --train_for_env_steps=2000000000 --algo=APPO --gamma=0.997 --use_rnn=True --rnn_num_layers=2 --num_workers=12 --num_envs_per_worker=2 --num_epochs=1 --rollout=32 --recurrence=32 --batch_size=4096 --actor_worker_gpus 0 --env_gpu_observations=False --num_policies=1 --with_pbt=False --max_grad_norm=0.0 --exploration_loss=symmetric_kl --exploration_loss_coeff=0.001 --megaverse_num_simulation_threads=1 --megaverse_num_envs_per_instance=32 --megaverse_num_agents_per_env=1 --megaverse_use_vulkan=True --policy_workers_per_policy=2 --reward_clip=30 --env=TowerBuilding --experiment=TowerBuilding To visualize the training results, use the enjoy_megaverse script: python -m megaverse_rl.enjoy_megaverse --algo=APPO --env=TowerBuilding --experiment=TowerBuilding --megaverse_num_envs_per_instance=1 --fps=20 --megaverse_use_vulkan=True Multiple experiments can be run in parallel with the runner module. megaverse_envs is an example runner script that runs atari envs with 4 seeds. python -m sample_factory.runner.run --run=megaverse_rl.runs.single_agent --runner=processes --max_parallel=2 --pause_between=1 --experiments_per_gpu=2 --num_gpus=1 Or you could run experiments on slurm: python -m sample_factory.runner.run --run=megaverse_rl.runs.single_agent --runner=slurm --slurm_workdir=./slurm_megaverse --experiment_suffix=slurm --slurm_gpus_per_job=1 --slurm_cpus_per_gpu=16 --slurm_sbatch_template=./sample_factory/runner/slurm/sbatch_template.sh --pause_between=1 --slurm_print_only=False","title":"Running Experiments"},{"location":"environment-integrations/megaverse/#results","text":"","title":"Results"},{"location":"environment-integrations/megaverse/#reports","text":"We trained models in the TowerBuilding environment in SF2 with single agent per env. https://wandb.ai/wmfrank/megaverse-benchmark/reports/Megaverse-trained-Sample-Factory--VmlldzoyNTAxMDUz","title":"Reports"},{"location":"environment-integrations/megaverse/#models","text":"An example APPO model trained on Megaverse environments is uploaded to the HuggingFace Hub. The models have all been trained for 2G steps. Environment HuggingFace Hub Models TowerBuilding https://huggingface.co/wmFrank/sample-factory-2-megaverse","title":"Models"},{"location":"environment-integrations/megaverse/#videos","text":"","title":"Videos"},{"location":"environment-integrations/megaverse/#tower-building-with-single-agent","text":"","title":"Tower Building with single agent"},{"location":"environment-integrations/megaverse/#tower-building-with-four-agents","text":"","title":"Tower Building with four agents"},{"location":"environment-integrations/mujoco/","text":"MuJoCo Integrations \u00b6 Installation \u00b6 Install Sample-Factory with MuJoCo dependencies with PyPI: pip install sample-factory[mujoco] Running Experiments \u00b6 Run MuJoCo experiments with the scripts in sf_examples.mujoco_examples . The default parameters have been chosen to match CleanRL's results in the report below. To train a model in the Ant-v4 enviornment: python -m sf_examples.mujoco_examples.train_mujoco --algo=APPO --env=mujoco_ant --experiment=<experiment_name> To visualize the training results, use the enjoy_mujoco script: python -m sf_examples.mujoco_examples.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<experiment_name> Multiple experiments can be run in parallel with the runner module. mujoco_all_envs is an example runner script that runs all mujoco envs with 10 seeds. python -m sample_factory.runner.run --run=sf_examples.mujoco_examples.experiments.mujoco_all_envs --runner=processes --max_parallel=4 --pause_between=1 --experiments_per_gpu=10000 --num_gpus=1 --experiment_suffix=0 List of Supported Environments \u00b6 Specify the environment to run with the --env command line parameter. The following MuJoCo v4 environments are supported out of the box, and more enviornments can be added as needed in sf_examples.mujoco_examples.mujoco.mujoco_utils MuJoCo Environment Name Sample-Factory Command Line Parameter Ant-v4 mujoco_ant HalfCheetah-v4 mujoco_halfcheetah Hopper-v4 mujoco_hopper Humanoid-v4 mujoco_humanoid Walker2d-v4 mujoco_walker InvertedDoublePendulum-v4 mujoco_doublependulum InvertedPendulum-v4 mujoco_pendulum Reacher-v4 mujoco_reacher Swimmer-v4 mujoco_swimmer Results \u00b6 Reports \u00b6 Sample-Factory was benchmarked on MuJoCo against CleanRL. Sample-Factory was able to achieve similar sample efficiency as CleanRL using the same parameters. https://wandb.ai/andrewzhang505/sample_factory/reports/MuJoCo-Sample-Factory-vs-CleanRL-w-o-EnvPool--VmlldzoyMjMyMTQ0 Sample-Factory can run experiments synchronously or asynchronously, with asynchronous execution usually having worse sample efficiency but runs faster. MuJoCo's environments were compared using the two modes in Sample-Factory https://wandb.ai/andrewzhang505/sample_factory/reports/MuJoCo-Synchronous-vs-Asynchronous--VmlldzoyMzEzNDUz Sample-Factory comparison with CleanRL in terms of wall time. Both experiments are run on a 16 core machine with 1 GPU. Sample-Factory was able to complete 10M samples 5 times as fast as CleanRL https://wandb.ai/andrewzhang505/sample_factory/reports/MuJoCo-Sample-Factory-vs-CleanRL-Wall-Time--VmlldzoyMzg2MDA3 Models \u00b6 Various APPO models trained on MuJoCo environments are uploaded to the HuggingFace Hub. The models have all been trained for 10M steps. Videos of the agents after training can be found on the HuggingFace Hub. The models below are the best models from the experiment against CleanRL above. The evaluation metrics here are obtained by running the model 10 times. Environment HuggingFace Hub Models Evaluation Metrics Ant-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-ant 5876.09 \u00b1 166.99 HalfCheetah-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-halfcheetah 6262.56 \u00b1 67.29 Humanoid-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-humanoid 5439.48 \u00b1 1314.24 Walker2d-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-walker 5487.74 \u00b1 48.96 Hopper-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-hopper 2793.44 \u00b1 642.58 InvertedDoublePendulum-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-doublependulum 9350.13 \u00b1 1.31 InvertedPendulum-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-pendulum 1000.00 \u00b1 0.00 Reacher-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-reacher -4.53 \u00b1 1.79 Swimmer-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-swimmer 117.28 \u00b1 2.91 Videos \u00b6 Below are some video examples of agents in various MuJoCo envioronments. Videos for all enviornments can be found in the HuggingFace Hub pages linked above. HalfCheetah-v4 \u00b6 Ant-v4 \u00b6 InvertedDoublePendulum-v4 \u00b6","title":"MuJoCo Integrations"},{"location":"environment-integrations/mujoco/#mujoco-integrations","text":"","title":"MuJoCo Integrations"},{"location":"environment-integrations/mujoco/#installation","text":"Install Sample-Factory with MuJoCo dependencies with PyPI: pip install sample-factory[mujoco]","title":"Installation"},{"location":"environment-integrations/mujoco/#running-experiments","text":"Run MuJoCo experiments with the scripts in sf_examples.mujoco_examples . The default parameters have been chosen to match CleanRL's results in the report below. To train a model in the Ant-v4 enviornment: python -m sf_examples.mujoco_examples.train_mujoco --algo=APPO --env=mujoco_ant --experiment=<experiment_name> To visualize the training results, use the enjoy_mujoco script: python -m sf_examples.mujoco_examples.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<experiment_name> Multiple experiments can be run in parallel with the runner module. mujoco_all_envs is an example runner script that runs all mujoco envs with 10 seeds. python -m sample_factory.runner.run --run=sf_examples.mujoco_examples.experiments.mujoco_all_envs --runner=processes --max_parallel=4 --pause_between=1 --experiments_per_gpu=10000 --num_gpus=1 --experiment_suffix=0","title":"Running Experiments"},{"location":"environment-integrations/mujoco/#list-of-supported-environments","text":"Specify the environment to run with the --env command line parameter. The following MuJoCo v4 environments are supported out of the box, and more enviornments can be added as needed in sf_examples.mujoco_examples.mujoco.mujoco_utils MuJoCo Environment Name Sample-Factory Command Line Parameter Ant-v4 mujoco_ant HalfCheetah-v4 mujoco_halfcheetah Hopper-v4 mujoco_hopper Humanoid-v4 mujoco_humanoid Walker2d-v4 mujoco_walker InvertedDoublePendulum-v4 mujoco_doublependulum InvertedPendulum-v4 mujoco_pendulum Reacher-v4 mujoco_reacher Swimmer-v4 mujoco_swimmer","title":"List of Supported Environments"},{"location":"environment-integrations/mujoco/#results","text":"","title":"Results"},{"location":"environment-integrations/mujoco/#reports","text":"Sample-Factory was benchmarked on MuJoCo against CleanRL. Sample-Factory was able to achieve similar sample efficiency as CleanRL using the same parameters. https://wandb.ai/andrewzhang505/sample_factory/reports/MuJoCo-Sample-Factory-vs-CleanRL-w-o-EnvPool--VmlldzoyMjMyMTQ0 Sample-Factory can run experiments synchronously or asynchronously, with asynchronous execution usually having worse sample efficiency but runs faster. MuJoCo's environments were compared using the two modes in Sample-Factory https://wandb.ai/andrewzhang505/sample_factory/reports/MuJoCo-Synchronous-vs-Asynchronous--VmlldzoyMzEzNDUz Sample-Factory comparison with CleanRL in terms of wall time. Both experiments are run on a 16 core machine with 1 GPU. Sample-Factory was able to complete 10M samples 5 times as fast as CleanRL https://wandb.ai/andrewzhang505/sample_factory/reports/MuJoCo-Sample-Factory-vs-CleanRL-Wall-Time--VmlldzoyMzg2MDA3","title":"Reports"},{"location":"environment-integrations/mujoco/#models","text":"Various APPO models trained on MuJoCo environments are uploaded to the HuggingFace Hub. The models have all been trained for 10M steps. Videos of the agents after training can be found on the HuggingFace Hub. The models below are the best models from the experiment against CleanRL above. The evaluation metrics here are obtained by running the model 10 times. Environment HuggingFace Hub Models Evaluation Metrics Ant-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-ant 5876.09 \u00b1 166.99 HalfCheetah-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-halfcheetah 6262.56 \u00b1 67.29 Humanoid-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-humanoid 5439.48 \u00b1 1314.24 Walker2d-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-walker 5487.74 \u00b1 48.96 Hopper-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-hopper 2793.44 \u00b1 642.58 InvertedDoublePendulum-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-doublependulum 9350.13 \u00b1 1.31 InvertedPendulum-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-pendulum 1000.00 \u00b1 0.00 Reacher-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-reacher -4.53 \u00b1 1.79 Swimmer-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-swimmer 117.28 \u00b1 2.91","title":"Models"},{"location":"environment-integrations/mujoco/#videos","text":"Below are some video examples of agents in various MuJoCo envioronments. Videos for all enviornments can be found in the HuggingFace Hub pages linked above.","title":"Videos"},{"location":"environment-integrations/mujoco/#halfcheetah-v4","text":"","title":"HalfCheetah-v4"},{"location":"environment-integrations/mujoco/#ant-v4","text":"","title":"Ant-v4"},{"location":"environment-integrations/mujoco/#inverteddoublependulum-v4","text":"","title":"InvertedDoublePendulum-v4"},{"location":"environment-integrations/vizdoom/","text":"VizDoom Integrations \u00b6 Installation \u00b6 To install VizDoom just follow system setup instructions from the original repository ( VizDoom linux_deps ), after which the latest VizDoom can be installed from PyPI: pip install vizdoom Running Experiments \u00b6 Run MuJoCo experiments with the scripts in sf_examples.vizdoom_examples . Train for 4B env steps (also can be stopped at any time with Ctrl+C and resumed by using the same cmd). This is more or less optimal training setup for a 10-core machine. python -m sf_examples.vizdoom_examples.train_vizdoom --env=doom_battle --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --batch_size=2048 --wide_aspect_ratio=False --num_workers=20 --num_envs_per_worker=20 --num_policies=1 --experiment=doom_battle_w20_v20 Run at any point to visualize the experiment: python -m sf_examples.vizdoom_examples.enjoy_vizdoom --env=doom_battle --algo=APPO --experiment=doom_battle_w20_v20 Runner scripts are also provided in sf_examples.vizdoom_examples.experiments to run experiments in parallel or on slurm. Reproducing Paper Results \u00b6 Train on one of the 6 \"basic\" VizDoom environments: python -m sf_examples.vizdoom_examples.train_vizdoom --train_for_env_steps=500000000 --algo=APPO --env=doom_my_way_home --env_frameskip=4 --use_rnn=True --num_workers=36 --num_envs_per_worker=8 --num_policies=1 --batch_size=2048 --wide_aspect_ratio=False --experiment=doom_basic_envs Doom \"battle\" and \"battle2\" environments, 36-core server (72 logical cores) with 4 GPUs: python -m sf_examples.vizdoom_examples.train_vizdoom --env=doom_battle --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=72 --num_envs_per_worker=8 --num_policies=1 --batch_size=2048 --wide_aspect_ratio=False --max_grad_norm=0.0 --experiment=doom_battle python -m sf_examples.vizdoom_examples.train_vizdoom --env=doom_battle2 --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=72 --num_envs_per_worker=8 --num_policies=1 --batch_size=2048 --wide_aspect_ratio=False --max_grad_norm=0.0 --experiment=doom_battle_2 Duel and deathmatch versus bots, population-based training, 36-core server: python -m sf_examples.vizdoom_examples.train_vizdoom --env=doom_duel_bots --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --reward_scale=0.5 --num_workers=72 --num_envs_per_worker=32 --num_policies=8 --batch_size=2048 --benchmark=False --res_w=128 --res_h=72 --wide_aspect_ratio=False --pbt_replace_reward_gap=0.2 --pbt_replace_reward_gap_absolute=3.0 --pbt_period_env_steps=5000000 --save_milestones_sec=1800 --with_pbt=True --experiment=doom_duel_bots python -m sf_examples.vizdoom_examples.train_vizdoom --env=doom_deathmatch_bots --train_for_seconds=3600000 --algo=APPO --use_rnn=True --gamma=0.995 --env_frameskip=2 --num_workers=80 --num_envs_per_worker=24 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --with_pbt=True --pbt_period_env_steps=5000000 --experiment=doom_deathmatch_bots Duel and deathmatch self-play, PBT, 36-core server: python -m sf_examples.vizdoom_examples.train_vizdoom --env=doom_duel --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --num_workers=72 --num_envs_per_worker=16 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --benchmark=False --pbt_replace_reward_gap=0.5 --pbt_replace_reward_gap_absolute=0.35 --pbt_period_env_steps=5000000 --with_pbt=True --pbt_start_mutation=100000000 --experiment=doom_duel_full python -m sf_examples.vizdoom_examples.train_vizdoom --env=doom_deathmatch_full --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --num_workers=72 --num_envs_per_worker=16 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --benchmark=False --pbt_replace_reward_gap=0.1 --pbt_replace_reward_gap_absolute=0.1 --pbt_period_env_steps=5000000 --with_pbt=True --pbt_start_mutation=100000000 --experiment=doom_deathmatch_full Reproducing benchmarking results: This achieves 50K+ framerate on a 10-core machine (Intel Core i9-7900X): python -m sf_examples.vizdoom_examples.train_vizdoom --env=doom_benchmark --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=20 --num_envs_per_worker=32 --num_policies=1 --batch_size=4096 --experiment=doom_battle_appo_fps_20_32 --res_w=128 --res_h=72 --wide_aspect_ratio=False --policy_workers_per_policy=2 --worker_num_splits=2 This achieves 100K+ framerate on a 36-core machine: python -m sf_examples.vizdoom_examples.train_vizdoom --env=doom_benchmark --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=72 --num_envs_per_worker=24 --num_policies=1 --batch_size=8192 --wide_aspect_ratio=False --experiment=doom_battle_appo_w72_v24 --policy_workers_per_policy=2 Results \u00b6 Reports \u00b6 We reproduced the paper results in SF2 in the Battle and Battle2 and compared the results using input normalization. Input normalization has improved results in the Battle environment. This experiment with input normalization was run with sf_examples.vizdoom_examples.experiments.sf2_doom_battle_envs . Note that normalize_input=True is set compared to the results from the paper https://wandb.ai/andrewzhang505/sample_factory/reports/VizDoom-Battle-Environments--VmlldzoyMzcyODQx In SF2's bot environments (deathmatch_bots and duel_bots), we trained the agents against randomly generated bots as opposed to a curriculum of increasing bot difficulty. This is because the ViZDoom environment no longer provides the bots used in the curriculum, and SF2 no longer requires the curriculum to train properly. However, due to the differences in bot difficulty, the current training results are no longer comparable to the paper. An example training curve on deathmatch_bots with the same parameters as in the paper is shown below: https://wandb.ai/andrewzhang505/sample_factory/reports/ViZDoom-Deathmatch-Bots--VmlldzoyNzY2NDI1 Models \u00b6 The models below are the best models from the input normalization experiment above. The evaluation metrics here are obtained by running the model 10 times. Environment HuggingFace Hub Models Evaluation Metrics Battle https://huggingface.co/andrewzhang505/sample-factory-2-doom-battle 59.37 \u00b1 3.93 Battle2 https://huggingface.co/andrewzhang505/sample-factory-2-doom-battle2 36.40 \u00b1 4.20 Videos \u00b6 Doom Battle \u00b6 Doom Battle2 \u00b6","title":"VizDoom Integrations"},{"location":"environment-integrations/vizdoom/#vizdoom-integrations","text":"","title":"VizDoom Integrations"},{"location":"environment-integrations/vizdoom/#installation","text":"To install VizDoom just follow system setup instructions from the original repository ( VizDoom linux_deps ), after which the latest VizDoom can be installed from PyPI: pip install vizdoom","title":"Installation"},{"location":"environment-integrations/vizdoom/#running-experiments","text":"Run MuJoCo experiments with the scripts in sf_examples.vizdoom_examples . Train for 4B env steps (also can be stopped at any time with Ctrl+C and resumed by using the same cmd). This is more or less optimal training setup for a 10-core machine. python -m sf_examples.vizdoom_examples.train_vizdoom --env=doom_battle --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --batch_size=2048 --wide_aspect_ratio=False --num_workers=20 --num_envs_per_worker=20 --num_policies=1 --experiment=doom_battle_w20_v20 Run at any point to visualize the experiment: python -m sf_examples.vizdoom_examples.enjoy_vizdoom --env=doom_battle --algo=APPO --experiment=doom_battle_w20_v20 Runner scripts are also provided in sf_examples.vizdoom_examples.experiments to run experiments in parallel or on slurm.","title":"Running Experiments"},{"location":"environment-integrations/vizdoom/#reproducing-paper-results","text":"Train on one of the 6 \"basic\" VizDoom environments: python -m sf_examples.vizdoom_examples.train_vizdoom --train_for_env_steps=500000000 --algo=APPO --env=doom_my_way_home --env_frameskip=4 --use_rnn=True --num_workers=36 --num_envs_per_worker=8 --num_policies=1 --batch_size=2048 --wide_aspect_ratio=False --experiment=doom_basic_envs Doom \"battle\" and \"battle2\" environments, 36-core server (72 logical cores) with 4 GPUs: python -m sf_examples.vizdoom_examples.train_vizdoom --env=doom_battle --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=72 --num_envs_per_worker=8 --num_policies=1 --batch_size=2048 --wide_aspect_ratio=False --max_grad_norm=0.0 --experiment=doom_battle python -m sf_examples.vizdoom_examples.train_vizdoom --env=doom_battle2 --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=72 --num_envs_per_worker=8 --num_policies=1 --batch_size=2048 --wide_aspect_ratio=False --max_grad_norm=0.0 --experiment=doom_battle_2 Duel and deathmatch versus bots, population-based training, 36-core server: python -m sf_examples.vizdoom_examples.train_vizdoom --env=doom_duel_bots --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --reward_scale=0.5 --num_workers=72 --num_envs_per_worker=32 --num_policies=8 --batch_size=2048 --benchmark=False --res_w=128 --res_h=72 --wide_aspect_ratio=False --pbt_replace_reward_gap=0.2 --pbt_replace_reward_gap_absolute=3.0 --pbt_period_env_steps=5000000 --save_milestones_sec=1800 --with_pbt=True --experiment=doom_duel_bots python -m sf_examples.vizdoom_examples.train_vizdoom --env=doom_deathmatch_bots --train_for_seconds=3600000 --algo=APPO --use_rnn=True --gamma=0.995 --env_frameskip=2 --num_workers=80 --num_envs_per_worker=24 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --with_pbt=True --pbt_period_env_steps=5000000 --experiment=doom_deathmatch_bots Duel and deathmatch self-play, PBT, 36-core server: python -m sf_examples.vizdoom_examples.train_vizdoom --env=doom_duel --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --num_workers=72 --num_envs_per_worker=16 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --benchmark=False --pbt_replace_reward_gap=0.5 --pbt_replace_reward_gap_absolute=0.35 --pbt_period_env_steps=5000000 --with_pbt=True --pbt_start_mutation=100000000 --experiment=doom_duel_full python -m sf_examples.vizdoom_examples.train_vizdoom --env=doom_deathmatch_full --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --num_workers=72 --num_envs_per_worker=16 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --benchmark=False --pbt_replace_reward_gap=0.1 --pbt_replace_reward_gap_absolute=0.1 --pbt_period_env_steps=5000000 --with_pbt=True --pbt_start_mutation=100000000 --experiment=doom_deathmatch_full Reproducing benchmarking results: This achieves 50K+ framerate on a 10-core machine (Intel Core i9-7900X): python -m sf_examples.vizdoom_examples.train_vizdoom --env=doom_benchmark --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=20 --num_envs_per_worker=32 --num_policies=1 --batch_size=4096 --experiment=doom_battle_appo_fps_20_32 --res_w=128 --res_h=72 --wide_aspect_ratio=False --policy_workers_per_policy=2 --worker_num_splits=2 This achieves 100K+ framerate on a 36-core machine: python -m sf_examples.vizdoom_examples.train_vizdoom --env=doom_benchmark --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=72 --num_envs_per_worker=24 --num_policies=1 --batch_size=8192 --wide_aspect_ratio=False --experiment=doom_battle_appo_w72_v24 --policy_workers_per_policy=2","title":"Reproducing Paper Results"},{"location":"environment-integrations/vizdoom/#results","text":"","title":"Results"},{"location":"environment-integrations/vizdoom/#reports","text":"We reproduced the paper results in SF2 in the Battle and Battle2 and compared the results using input normalization. Input normalization has improved results in the Battle environment. This experiment with input normalization was run with sf_examples.vizdoom_examples.experiments.sf2_doom_battle_envs . Note that normalize_input=True is set compared to the results from the paper https://wandb.ai/andrewzhang505/sample_factory/reports/VizDoom-Battle-Environments--VmlldzoyMzcyODQx In SF2's bot environments (deathmatch_bots and duel_bots), we trained the agents against randomly generated bots as opposed to a curriculum of increasing bot difficulty. This is because the ViZDoom environment no longer provides the bots used in the curriculum, and SF2 no longer requires the curriculum to train properly. However, due to the differences in bot difficulty, the current training results are no longer comparable to the paper. An example training curve on deathmatch_bots with the same parameters as in the paper is shown below: https://wandb.ai/andrewzhang505/sample_factory/reports/ViZDoom-Deathmatch-Bots--VmlldzoyNzY2NDI1","title":"Reports"},{"location":"environment-integrations/vizdoom/#models","text":"The models below are the best models from the input normalization experiment above. The evaluation metrics here are obtained by running the model 10 times. Environment HuggingFace Hub Models Evaluation Metrics Battle https://huggingface.co/andrewzhang505/sample-factory-2-doom-battle 59.37 \u00b1 3.93 Battle2 https://huggingface.co/andrewzhang505/sample-factory-2-doom-battle2 36.40 \u00b1 4.20","title":"Models"},{"location":"environment-integrations/vizdoom/#videos","text":"","title":"Videos"},{"location":"environment-integrations/vizdoom/#doom-battle","text":"","title":"Doom Battle"},{"location":"environment-integrations/vizdoom/#doom-battle2","text":"","title":"Doom Battle2"},{"location":"get-started/basic-usage/","text":"Basic Usage \u00b6 Using Sample Factory \u00b6 Once Sample Factpry is installed, it defines two main entry points, one for training, and one for algorithm evaluation: * sample_factory.algorithms.appo.train_appo * sample_factory.algorithms.appo.enjoy_appo Some environments, such as VizDoom, DMLab, and Atari, are added to the env registry in the default installation, so training on these environments is as simple as providing basic configuration parameters. I.e. to train and evaluate on the most basic VizDoom environment: python -m sample_factory.algorithms.appo.train_appo --env=doom_basic --algo=APPO --train_for_env_steps=3000000 --num_workers=20 --num_envs_per_worker=20 --experiment=doom_basic python -m sample_factory.algorithms.appo.enjoy_appo --env=doom_basic --algo=APPO --experiment=doom_basic Configuration \u00b6 Sample Factory experiments are configured via command line parameters. The following command will print the help message for the algorithm-environment combination: python -m sample_factory.algorithms.appo.train_appo --algo=APPO --env=doom_battle --experiment=your_experiment --help This will print the full list of parameters, their descriptions, and their default values. Replace doom_battle with a different environment name (i.e. atari_breakout ) to get information about parameters specific to this particular environment. Once the new experiment is started, a directory containing experiment-related files is created in --train_dir location (or ./train_dir in cwd if --train_dir is not passed from command line). This directory contains a file cfg.json where all the experiment parameters are saved (including those instantiated from their default values). Most default parameter values and their help strings are defined in sample_factory/algorithms/algorithm.py and sample_factory/algorithms/appo/appo.py . Besides that, additional parameters can be defined for specific families of environments. The key parameters are: --algo (required) algorithm to use, pass value APPO to train agents with fast Async PPO. --env (required) full name that uniquely identifies the environment, starting with the env family prefix (e.g. doom_ , dmlab_ or atari_ for built-in Sample Factory envs). E.g. doom_battle or atari_breakout . --experiment (required) a name that uniquely identifies the experiment. E.g. --experiment=my_experiment . If the experiment folder with the name already exists the experiment will be resumed ! Resuming experiments after a stop is the default behavior in Sample Factory. The parameters passed from command line are taken into account, unspecified parameters will be loaded from the existing experiment cfg.json file. If you want to start a new experiment, delete the old experiment folder or change the experiment name. --train_dir location for all experiments folders, defaults to ./train_dir . --num_workers defaults to number of logical cores in the system, which will give the best throughput in most scenarios. --num_envs_per_worker will greatly affect the performance. Large values (20-30) improve hardware utilization but increase memory usage and policy lag. See example command lines below to find a value that works for your system. Must be even for the double-buffered sampling to work. Disable double-buffered sampling by setting --worker_num_splits=1 to use odd number of envs per worker (e.g. 1 env per worker). (Default: 2) Configuring actor & critic architectures \u00b6 sample_factory/algorithms/algorithm.py contains parameters that allow users to customize the architectures of neural networks involved in the training process. Sample Factory includes a few popular NN architectures for RL, such as shallow convnets for Atari and VizDoom, deeper ResNet models for DMLab, MLPs for continuous control tasks. CLI parameters allow users to choose between these existing architectures, as well as specify the type of the policy core (LSTM/GRU/feed-forward), nonlinearities, etc. Consult experiment-specific cfg.json and the source code for full list of parameters. sample_factory.envs.dmlab.dmlab_model and sample_factory.envs.doom.doom_model demonstrate how to handle environment-specific additional input spaces (e.g. natural language and/or numerical vector inputs). Script sample_factory_examples/train_custom_env_custom_model.py demonstrates how users can define a fully custom environment-specific encoder. Whenever a fully custom actor-critic architecture is required, users are welcome to override _ActorCriticBase following examples in sample_factory/algorithms/appo/model.py .","title":"Basic Usage"},{"location":"get-started/basic-usage/#basic-usage","text":"","title":"Basic Usage"},{"location":"get-started/basic-usage/#using-sample-factory","text":"Once Sample Factpry is installed, it defines two main entry points, one for training, and one for algorithm evaluation: * sample_factory.algorithms.appo.train_appo * sample_factory.algorithms.appo.enjoy_appo Some environments, such as VizDoom, DMLab, and Atari, are added to the env registry in the default installation, so training on these environments is as simple as providing basic configuration parameters. I.e. to train and evaluate on the most basic VizDoom environment: python -m sample_factory.algorithms.appo.train_appo --env=doom_basic --algo=APPO --train_for_env_steps=3000000 --num_workers=20 --num_envs_per_worker=20 --experiment=doom_basic python -m sample_factory.algorithms.appo.enjoy_appo --env=doom_basic --algo=APPO --experiment=doom_basic","title":"Using Sample Factory"},{"location":"get-started/basic-usage/#configuration","text":"Sample Factory experiments are configured via command line parameters. The following command will print the help message for the algorithm-environment combination: python -m sample_factory.algorithms.appo.train_appo --algo=APPO --env=doom_battle --experiment=your_experiment --help This will print the full list of parameters, their descriptions, and their default values. Replace doom_battle with a different environment name (i.e. atari_breakout ) to get information about parameters specific to this particular environment. Once the new experiment is started, a directory containing experiment-related files is created in --train_dir location (or ./train_dir in cwd if --train_dir is not passed from command line). This directory contains a file cfg.json where all the experiment parameters are saved (including those instantiated from their default values). Most default parameter values and their help strings are defined in sample_factory/algorithms/algorithm.py and sample_factory/algorithms/appo/appo.py . Besides that, additional parameters can be defined for specific families of environments. The key parameters are: --algo (required) algorithm to use, pass value APPO to train agents with fast Async PPO. --env (required) full name that uniquely identifies the environment, starting with the env family prefix (e.g. doom_ , dmlab_ or atari_ for built-in Sample Factory envs). E.g. doom_battle or atari_breakout . --experiment (required) a name that uniquely identifies the experiment. E.g. --experiment=my_experiment . If the experiment folder with the name already exists the experiment will be resumed ! Resuming experiments after a stop is the default behavior in Sample Factory. The parameters passed from command line are taken into account, unspecified parameters will be loaded from the existing experiment cfg.json file. If you want to start a new experiment, delete the old experiment folder or change the experiment name. --train_dir location for all experiments folders, defaults to ./train_dir . --num_workers defaults to number of logical cores in the system, which will give the best throughput in most scenarios. --num_envs_per_worker will greatly affect the performance. Large values (20-30) improve hardware utilization but increase memory usage and policy lag. See example command lines below to find a value that works for your system. Must be even for the double-buffered sampling to work. Disable double-buffered sampling by setting --worker_num_splits=1 to use odd number of envs per worker (e.g. 1 env per worker). (Default: 2)","title":"Configuration"},{"location":"get-started/basic-usage/#configuring-actor-critic-architectures","text":"sample_factory/algorithms/algorithm.py contains parameters that allow users to customize the architectures of neural networks involved in the training process. Sample Factory includes a few popular NN architectures for RL, such as shallow convnets for Atari and VizDoom, deeper ResNet models for DMLab, MLPs for continuous control tasks. CLI parameters allow users to choose between these existing architectures, as well as specify the type of the policy core (LSTM/GRU/feed-forward), nonlinearities, etc. Consult experiment-specific cfg.json and the source code for full list of parameters. sample_factory.envs.dmlab.dmlab_model and sample_factory.envs.doom.doom_model demonstrate how to handle environment-specific additional input spaces (e.g. natural language and/or numerical vector inputs). Script sample_factory_examples/train_custom_env_custom_model.py demonstrates how users can define a fully custom environment-specific encoder. Whenever a fully custom actor-critic architecture is required, users are welcome to override _ActorCriticBase following examples in sample_factory/algorithms/appo/model.py .","title":"Configuring actor &amp; critic architectures"},{"location":"get-started/huggingface/","text":"HuggingFace Hub Integration \u00b6 Sample Factory has integrations with HuggingFace Hub to push models with evaluation results and training metrics to the hub. Setting Up \u00b6 The HuggingFace Hub requires git lfs to download model files. sudo apt install git-lfs git lfs install To upload files to the HuggingFace Hub, you need to log in to your HuggingFace account with: huggingface-cli login As part of the huggingface-cli login , you should generate a token with write access at https://huggingface.co/settings/tokens Downloading Models \u00b6 Using the load_from_hub Scipt \u00b6 To download a model from the HuggingFace Hub to use with Sample-Factory, use the load_from_hub script: python -m sample_factory.huggingface.load_from_hub -r <HuggingFace_repo_id> -d <train_dir_path> The command line arguments are: -r : The repo ID for the HF repository to download. The repo ID should be in the format <username>/<repo_name> -d : An optional argument to specify the directory to save the experiment to. Defaults to ./train_dir which will save the repo to ./train_dir/<repo_name> Download Model Repository Directly \u00b6 HuggingFace repositories can be downloaded directly using git clone : git clone <URL of HuggingFace Repo> Using Downloaded Models with Sample-Factory \u00b6 After downloading the model, you can run the models in the repo with the enjoy script corresponding to your environment. For example, if you are downloading a mujoco-ant model, it can be run with: python -m sf_examples.mujoco_examples.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<repo_name> --train_dir=./train_dir Note, you may have to specify the --train_dir if your local train_dir has a different path than the one in the cfg.json Uploading Models \u00b6 Using enjoy.py \u00b6 You can upload your models to the Hub using your environment's enjoy script with the --push_to_hub flag. Uploading using enjoy can also generate evaluation metrics and a replay video. The evaluation metrics are generated by running your model on the specified environment for a number of episodes and reporting the mean and std reward of those runs. Other relevant command line arguments are: --hf_username : Your HuggingFace username --hf_repository : The repository to push to. The model will be saved to https://huggingface.co/hf_username/hf_repository --max_num_episodes : Number of episodes to evaluate on before uploading. Used to generate evaluation metrics. It is recommended to use multiple episodes to generate an accurate mean and std. --max_num_frames : Number of frames to evaluate on before uploading. An alternative to max_num_episodes --no_render : A flag that disables rendering and showing the environment steps. It is recommended to set this flag to speed up the evaluation process. You can also save a video of the model during evaluation to upload to the hub with the --save_video flag --video_frames : The number of frames to be rendered in the video. Defaults to -1 which renders an entire episode --video_name : The name of the video to save as. If None , will save to replay.mp4 in your experiment directory For example: python -m sf_examples.mujoco_examples.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<repo_name> --train_dir=./train_dir --max_num_episodes=10 --push_to_hub --hf_username=<username> --hf_repository=<hf_repo_name> --save_video --no_render Using the push_to_hub Script \u00b6 If you want to upload without generating evaluation metrics or a replay video, you can use the push_to_hub script: python -m sample_factory.huggingface.push_to_hub -r <hf_repo_name> -u <hf_username> -d <experiment_dir_path> The command line arguments are: -r : The name of the repo to save on HF Hub. This is the same as hf_repository in the enjoy script -u : Your HuggingFace username -d : The full path to your experiment directory to upload","title":"HuggingFace Hub Integration"},{"location":"get-started/huggingface/#huggingface-hub-integration","text":"Sample Factory has integrations with HuggingFace Hub to push models with evaluation results and training metrics to the hub.","title":"HuggingFace Hub Integration"},{"location":"get-started/huggingface/#setting-up","text":"The HuggingFace Hub requires git lfs to download model files. sudo apt install git-lfs git lfs install To upload files to the HuggingFace Hub, you need to log in to your HuggingFace account with: huggingface-cli login As part of the huggingface-cli login , you should generate a token with write access at https://huggingface.co/settings/tokens","title":"Setting Up"},{"location":"get-started/huggingface/#downloading-models","text":"","title":"Downloading Models"},{"location":"get-started/huggingface/#using-the-load_from_hub-scipt","text":"To download a model from the HuggingFace Hub to use with Sample-Factory, use the load_from_hub script: python -m sample_factory.huggingface.load_from_hub -r <HuggingFace_repo_id> -d <train_dir_path> The command line arguments are: -r : The repo ID for the HF repository to download. The repo ID should be in the format <username>/<repo_name> -d : An optional argument to specify the directory to save the experiment to. Defaults to ./train_dir which will save the repo to ./train_dir/<repo_name>","title":"Using the load_from_hub Scipt"},{"location":"get-started/huggingface/#download-model-repository-directly","text":"HuggingFace repositories can be downloaded directly using git clone : git clone <URL of HuggingFace Repo>","title":"Download Model Repository Directly"},{"location":"get-started/huggingface/#using-downloaded-models-with-sample-factory","text":"After downloading the model, you can run the models in the repo with the enjoy script corresponding to your environment. For example, if you are downloading a mujoco-ant model, it can be run with: python -m sf_examples.mujoco_examples.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<repo_name> --train_dir=./train_dir Note, you may have to specify the --train_dir if your local train_dir has a different path than the one in the cfg.json","title":"Using Downloaded Models with Sample-Factory"},{"location":"get-started/huggingface/#uploading-models","text":"","title":"Uploading Models"},{"location":"get-started/huggingface/#using-enjoypy","text":"You can upload your models to the Hub using your environment's enjoy script with the --push_to_hub flag. Uploading using enjoy can also generate evaluation metrics and a replay video. The evaluation metrics are generated by running your model on the specified environment for a number of episodes and reporting the mean and std reward of those runs. Other relevant command line arguments are: --hf_username : Your HuggingFace username --hf_repository : The repository to push to. The model will be saved to https://huggingface.co/hf_username/hf_repository --max_num_episodes : Number of episodes to evaluate on before uploading. Used to generate evaluation metrics. It is recommended to use multiple episodes to generate an accurate mean and std. --max_num_frames : Number of frames to evaluate on before uploading. An alternative to max_num_episodes --no_render : A flag that disables rendering and showing the environment steps. It is recommended to set this flag to speed up the evaluation process. You can also save a video of the model during evaluation to upload to the hub with the --save_video flag --video_frames : The number of frames to be rendered in the video. Defaults to -1 which renders an entire episode --video_name : The name of the video to save as. If None , will save to replay.mp4 in your experiment directory For example: python -m sf_examples.mujoco_examples.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<repo_name> --train_dir=./train_dir --max_num_episodes=10 --push_to_hub --hf_username=<username> --hf_repository=<hf_repo_name> --save_video --no_render","title":"Using enjoy.py"},{"location":"get-started/huggingface/#using-the-push_to_hub-script","text":"If you want to upload without generating evaluation metrics or a replay video, you can use the push_to_hub script: python -m sample_factory.huggingface.push_to_hub -r <hf_repo_name> -u <hf_username> -d <experiment_dir_path> The command line arguments are: -r : The name of the repo to save on HF Hub. This is the same as hf_repository in the enjoy script -u : Your HuggingFace username -d : The full path to your experiment directory to upload","title":"Using the push_to_hub Script"},{"location":"get-started/installation/","text":"Installation \u00b6 Using pip \u00b6 Just install from PyPI: pip install sample-factory SF is known to work on Linux and macOS. There is no Windows support at this time. Environment support \u00b6 Sample Factory has a runtime environment registry for families of environments . A family of environments is defined by a name prefix (i.e. atari_ or doom_ ) and a function that creates an instance of the environment given its full name, including the prefix (i.e. atari_breakout ). Registering families of environments allows the user to add and override configuration parameters (such as resolution, frameskip, default model type, etc.) for the whole family of environments, i.e. all VizDoom envs can share their basic configuration parameters that don't need to be specified for each experiment. Custom user-defined environment families and models can be added to the registry, see this example: sample_factory_examples/train_custom_env_custom_model.py Script sample_factory_examples/train_gym_env.py demonstrates how Sample Factory can be used with an environment defined in OpenAI Gym. Sample Factory comes with a particularly comprehensive support for VizDoom and DMLab, see below. VizDoom \u00b6 To install VizDoom just follow system setup instructions from the original repository ( VizDoom linux_deps ), after which the latest VizDoom can be installed from PyPI: pip install vizdoom . Version 1.1.9 or above is recommended as it fixes bugs related to multi-agent training. DMLab \u00b6 Follow installation instructions from DMLab Github . pip install dm_env To train on DMLab-30 you will need brady_konkle_oliva2008 dataset . To significantly speed up training on DMLab-30 consider downloading our dataset of pre-generated environment layouts (see paper for details). Command lines for running experiments with these datasets are provided in the sections below. Atari \u00b6 ALE envs are supported out-of-the-box, although the existing wrappers and hyperparameters aren't well optimized for sample efficiency in Atari. Tuned Atari training examples would be a welcome contribution. Since ~2022 some extra steps might be required to install atari: pip install \"gym[atari,accept-rom-license]\" Custom multi-agent environments \u00b6 Multi-agent environments are expected to return lists of observations/dones/rewards (one item for every agent). It is expected that a multi-agent env exposes a property or a member variable num_agents that the algorithm uses to allocate the right amount of memory during startup. Multi-agent environments require auto-reset. I.e. they reset a particular agent when the corresponding done flag is True and return the first observation of the next episode (because we have no use for the last observation of the previous episode, we do not act based on it). See multi_agent_wrapper.py for example. For simplicity Sample Factory actually treats all environments as multi-agent, i.e. single-agent environments are automatically treated as multi-agent environments with one agent. Sample Factory uses this function to check if the environment is multi-agent. Make sure your environment provides the num_agents member: def is_multiagent_env ( env ): is_multiagent = hasattr ( env , 'num_agents' ) and env . num_agents > 1 if hasattr ( env , 'is_multiagent' ): is_multiagent = env . is_multiagent return is_multiagent","title":"Installation"},{"location":"get-started/installation/#installation","text":"","title":"Installation"},{"location":"get-started/installation/#using-pip","text":"Just install from PyPI: pip install sample-factory SF is known to work on Linux and macOS. There is no Windows support at this time.","title":"Using pip"},{"location":"get-started/installation/#environment-support","text":"Sample Factory has a runtime environment registry for families of environments . A family of environments is defined by a name prefix (i.e. atari_ or doom_ ) and a function that creates an instance of the environment given its full name, including the prefix (i.e. atari_breakout ). Registering families of environments allows the user to add and override configuration parameters (such as resolution, frameskip, default model type, etc.) for the whole family of environments, i.e. all VizDoom envs can share their basic configuration parameters that don't need to be specified for each experiment. Custom user-defined environment families and models can be added to the registry, see this example: sample_factory_examples/train_custom_env_custom_model.py Script sample_factory_examples/train_gym_env.py demonstrates how Sample Factory can be used with an environment defined in OpenAI Gym. Sample Factory comes with a particularly comprehensive support for VizDoom and DMLab, see below.","title":"Environment support"},{"location":"get-started/installation/#vizdoom","text":"To install VizDoom just follow system setup instructions from the original repository ( VizDoom linux_deps ), after which the latest VizDoom can be installed from PyPI: pip install vizdoom . Version 1.1.9 or above is recommended as it fixes bugs related to multi-agent training.","title":"VizDoom"},{"location":"get-started/installation/#dmlab","text":"Follow installation instructions from DMLab Github . pip install dm_env To train on DMLab-30 you will need brady_konkle_oliva2008 dataset . To significantly speed up training on DMLab-30 consider downloading our dataset of pre-generated environment layouts (see paper for details). Command lines for running experiments with these datasets are provided in the sections below.","title":"DMLab"},{"location":"get-started/installation/#atari","text":"ALE envs are supported out-of-the-box, although the existing wrappers and hyperparameters aren't well optimized for sample efficiency in Atari. Tuned Atari training examples would be a welcome contribution. Since ~2022 some extra steps might be required to install atari: pip install \"gym[atari,accept-rom-license]\"","title":"Atari"},{"location":"get-started/installation/#custom-multi-agent-environments","text":"Multi-agent environments are expected to return lists of observations/dones/rewards (one item for every agent). It is expected that a multi-agent env exposes a property or a member variable num_agents that the algorithm uses to allocate the right amount of memory during startup. Multi-agent environments require auto-reset. I.e. they reset a particular agent when the corresponding done flag is True and return the first observation of the next episode (because we have no use for the last observation of the previous episode, we do not act based on it). See multi_agent_wrapper.py for example. For simplicity Sample Factory actually treats all environments as multi-agent, i.e. single-agent environments are automatically treated as multi-agent environments with one agent. Sample Factory uses this function to check if the environment is multi-agent. Make sure your environment provides the num_agents member: def is_multiagent_env ( env ): is_multiagent = hasattr ( env , 'num_agents' ) and env . num_agents > 1 if hasattr ( env , 'is_multiagent' ): is_multiagent = env . is_multiagent return is_multiagent","title":"Custom multi-agent environments"},{"location":"get-started/running-experiments/","text":"Running Experiments \u00b6 Here we provide command lines that can be used to reproduce the experiments from the paper, which also serve as an example on how to configure large-scale RL experiments. DMLab \u00b6 DMLab-30 run on a 36-core server with 4 GPUs: python -m sample_factory.algorithms.appo.train_appo --env=dmlab_30 --train_for_seconds=3600000 --algo=APPO --gamma=0.99 --use_rnn=True --num_workers=90 --num_envs_per_worker=12 --ppo_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --benchmark=False --ppo_epochs=1 --max_grad_norm=0.0 --dmlab_renderer=software --decorrelate_experience_max_seconds=120 --reset_timeout_seconds=300 --encoder_custom=dmlab_instructions --encoder_type=resnet --encoder_subtype=resnet_impala --encoder_extra_fc_layers=1 --hidden_size=256 --nonlinearity=relu --rnn_type=lstm --dmlab_extended_action_set=True --num_policies=4 --pbt_replace_reward_gap=0.05 --pbt_replace_reward_gap_absolute=5.0 --pbt_period_env_steps=10000000 --pbt_start_mutation=100000000 --with_pbt=True --experiment=dmlab_30_resnet_4pbt_w90_v12 --dmlab_one_task_per_worker=True --set_workers_cpu_affinity=True --max_policy_lag=35 --pbt_target_objective=dmlab_target_objective --dmlab30_dataset=~/datasets/brady_konkle_oliva2008 --dmlab_use_level_cache=True --dmlab_level_cache_path=/home/user/dmlab_cache DMLab level cache \u00b6 Note --dmlab_level_cache_path parameter. This location will be used for level layout cache. Subsequent DMLab experiments on envs that require level generation will become faster since environment files from previous runs can be reused. Generating environment levels for the first time can be really slow, especially for the full multi-task benchmark like DMLab-30. On 36-core server generating enough environments for a 10B training session can take up to a week. We provide a dataset of pre-generated levels to make training on DMLab-30 easier. Download here . Monitoring training sessions \u00b6 Sample Factory uses Tensorboard summaries. Run Tensorboard to monitor your experiment: tensorboard --logdir=train_dir --port=6006 Additionally, we provide a helper script that has nice command line interface to monitor the experiment folders using wildcard masks: python -m sample_factory.tb '*custom_experiment*' '*another*custom*experiment_name' WandB support \u00b6 Sample Factory also supports experiment monitoring with Weights and Biases. In order to setup WandB locally run wandb login in the terminal ( https://docs.wandb.ai/quickstart#1.-set-up-wandb ) Example command line to run an experiment with WandB monitoring: python -m sample_factory.algorithms.appo.train_appo --env=doom_basic --algo=APPO --train_for_env_steps=30000000 --num_workers=20 --num_envs_per_worker=20 --experiment=doom_basic --with_wandb=True --wandb_user=<your_wandb_user> --wandb_tags test benchmark doom appo A total list of WandB settings: --with_wandb: Enables Weights and Biases integration (default: False) --wandb_user: WandB username (entity). Must be specified from command line! Also see https://docs.wandb.ai/quickstart#1.-set-up-wandb (default: None) --wandb_project: WandB \"Project\" (default: sample_factory) --wandb_group: WandB \"Group\" (to group your experiments). By default this is the name of the env. (default: None) --wandb_job_type: WandB job type (default: SF) --wandb_tags: [WANDB_TAGS [WANDB_TAGS ...]] Tags can help with finding experiments in WandB web console (default: []) Once the experiment is started the link to the monitored session is going to be available in the logs (or by searching in Wandb Web console). Runner interface \u00b6 Sample Factory provides a simple interface that allows users to run experiments with multiple seeds (or hyperparameter searches) with optimal distribution of work across GPUs. The configuration of such experiments is done through Python scripts. Here's an example runner script that we used to train agents for 6 basic VizDoom environments with 10 seeds each: from sample_factory.runner.run_description import RunDescription, Experiment, ParamGrid _params = ParamGrid([ ('seed', [0, 1111, 2222, 3333, 4444, 5555, 6666, 7777, 8888, 9999]), ('env', ['doom_my_way_home', 'doom_deadly_corridor', 'doom_defend_the_center', 'doom_defend_the_line', 'doom_health_gathering', 'doom_health_gathering_supreme']), ]) _experiments = [ Experiment( 'basic_envs_fs4', 'python -m sample_factory.algorithms.appo.train_appo --train_for_env_steps=500000000 --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=36 --num_envs_per_worker=8 --num_policies=1 --ppo_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --wide_aspect_ratio=False', _params.generate_params(randomize=False), ), ] RUN_DESCRIPTION = RunDescription('doom_basic_envs_appo', experiments=_experiments) Runner script should be importable (i.e. be in your project or in PYTHONPATH), and should define a single variable RUN_DESCRIPTION , which contains a list of experiments (each experiment can be a hyperparameter search), as well as some auxiliary parameters. When such a script is saved i.e. at myproject/train_10_seeds.py in your project using Sample Factory, you can use this command to execute it: python -m sample_factory.runner.run --run=myproject.train_10_seeds --runner=processes --max_parallel=12 --pause_between=10 --experiments_per_gpu=3 --num_gpus=4 This will cycle through the requested configurations, training 12 experiments at the same time, 3 per GPU on 4 GPUs using local OS-level parallelism. Runner supports other backends for parallel execution: --runner=slurm and --runner=ngc for Slurm and NGC support respectively. Individual experiments will be stored in train_dir/run_name so the whole experiment can be easily monitored with a single Tensorboard command. Find more information on runner API in runner/README.md . Dummy sampler \u00b6 This tool can be useful if you want to estimate the upper bound on performance of any reinforcement learning algorithm, i.e. how fast the environment can be sampled by a dumb random policy. This achieves 90000+ FPS on a 10-core workstation: python -m sample_factory.run_algorithm --algo=DUMMY_SAMPLER --env=doom_benchmark --num_workers=20 --num_envs_per_worker=1 --experiment=dummy_sampler --sample_env_frames=5000000 Tests \u00b6 To run unit tests execute ./all_tests.sh from the root of the repo. Consider installing VizDoom for a more comprehensive set of tests. Trained policies \u00b6 See a separate trained_policies/README.md . Caveats \u00b6 Multiplayer VizDoom environments can freeze your console sometimes, simple reset takes care of this Sometimes VizDoom instances don't clear their internal shared memory buffers used to communicate between Python and a Doom executable. The file descriptors for these buffers tend to pile up. rm /dev/shm/ViZDoom* will take care of this issue. It's best to use the standard --fps=35 to visualize VizDoom results. --fps=0 enables Async execution mode for the Doom environments, although the results are not always reproducible between sync and async modes. Multiplayer VizDoom environments are significantly slower than single-player envs because actual network communication between the environment instances is required which results in a lot of syscalls. For prototyping and testing consider single-player environments with bots instead. Vectors of environments on rollout (actor) workers are instantiated on the same CPU thread. This can create problems for certain types of environment that require global per-thread or per-process context (e.g. OpenGL context). The solution should be an environment wrapper that starts the environment in a separate thread (or process if that's required) and communicates. doom_multiagent_wrapper.py is an example, although not optimal.","title":"Running Experiments"},{"location":"get-started/running-experiments/#running-experiments","text":"Here we provide command lines that can be used to reproduce the experiments from the paper, which also serve as an example on how to configure large-scale RL experiments.","title":"Running Experiments"},{"location":"get-started/running-experiments/#dmlab","text":"DMLab-30 run on a 36-core server with 4 GPUs: python -m sample_factory.algorithms.appo.train_appo --env=dmlab_30 --train_for_seconds=3600000 --algo=APPO --gamma=0.99 --use_rnn=True --num_workers=90 --num_envs_per_worker=12 --ppo_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --benchmark=False --ppo_epochs=1 --max_grad_norm=0.0 --dmlab_renderer=software --decorrelate_experience_max_seconds=120 --reset_timeout_seconds=300 --encoder_custom=dmlab_instructions --encoder_type=resnet --encoder_subtype=resnet_impala --encoder_extra_fc_layers=1 --hidden_size=256 --nonlinearity=relu --rnn_type=lstm --dmlab_extended_action_set=True --num_policies=4 --pbt_replace_reward_gap=0.05 --pbt_replace_reward_gap_absolute=5.0 --pbt_period_env_steps=10000000 --pbt_start_mutation=100000000 --with_pbt=True --experiment=dmlab_30_resnet_4pbt_w90_v12 --dmlab_one_task_per_worker=True --set_workers_cpu_affinity=True --max_policy_lag=35 --pbt_target_objective=dmlab_target_objective --dmlab30_dataset=~/datasets/brady_konkle_oliva2008 --dmlab_use_level_cache=True --dmlab_level_cache_path=/home/user/dmlab_cache","title":"DMLab"},{"location":"get-started/running-experiments/#dmlab-level-cache","text":"Note --dmlab_level_cache_path parameter. This location will be used for level layout cache. Subsequent DMLab experiments on envs that require level generation will become faster since environment files from previous runs can be reused. Generating environment levels for the first time can be really slow, especially for the full multi-task benchmark like DMLab-30. On 36-core server generating enough environments for a 10B training session can take up to a week. We provide a dataset of pre-generated levels to make training on DMLab-30 easier. Download here .","title":"DMLab level cache"},{"location":"get-started/running-experiments/#monitoring-training-sessions","text":"Sample Factory uses Tensorboard summaries. Run Tensorboard to monitor your experiment: tensorboard --logdir=train_dir --port=6006 Additionally, we provide a helper script that has nice command line interface to monitor the experiment folders using wildcard masks: python -m sample_factory.tb '*custom_experiment*' '*another*custom*experiment_name'","title":"Monitoring training sessions"},{"location":"get-started/running-experiments/#wandb-support","text":"Sample Factory also supports experiment monitoring with Weights and Biases. In order to setup WandB locally run wandb login in the terminal ( https://docs.wandb.ai/quickstart#1.-set-up-wandb ) Example command line to run an experiment with WandB monitoring: python -m sample_factory.algorithms.appo.train_appo --env=doom_basic --algo=APPO --train_for_env_steps=30000000 --num_workers=20 --num_envs_per_worker=20 --experiment=doom_basic --with_wandb=True --wandb_user=<your_wandb_user> --wandb_tags test benchmark doom appo A total list of WandB settings: --with_wandb: Enables Weights and Biases integration (default: False) --wandb_user: WandB username (entity). Must be specified from command line! Also see https://docs.wandb.ai/quickstart#1.-set-up-wandb (default: None) --wandb_project: WandB \"Project\" (default: sample_factory) --wandb_group: WandB \"Group\" (to group your experiments). By default this is the name of the env. (default: None) --wandb_job_type: WandB job type (default: SF) --wandb_tags: [WANDB_TAGS [WANDB_TAGS ...]] Tags can help with finding experiments in WandB web console (default: []) Once the experiment is started the link to the monitored session is going to be available in the logs (or by searching in Wandb Web console).","title":"WandB support"},{"location":"get-started/running-experiments/#runner-interface","text":"Sample Factory provides a simple interface that allows users to run experiments with multiple seeds (or hyperparameter searches) with optimal distribution of work across GPUs. The configuration of such experiments is done through Python scripts. Here's an example runner script that we used to train agents for 6 basic VizDoom environments with 10 seeds each: from sample_factory.runner.run_description import RunDescription, Experiment, ParamGrid _params = ParamGrid([ ('seed', [0, 1111, 2222, 3333, 4444, 5555, 6666, 7777, 8888, 9999]), ('env', ['doom_my_way_home', 'doom_deadly_corridor', 'doom_defend_the_center', 'doom_defend_the_line', 'doom_health_gathering', 'doom_health_gathering_supreme']), ]) _experiments = [ Experiment( 'basic_envs_fs4', 'python -m sample_factory.algorithms.appo.train_appo --train_for_env_steps=500000000 --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=36 --num_envs_per_worker=8 --num_policies=1 --ppo_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --wide_aspect_ratio=False', _params.generate_params(randomize=False), ), ] RUN_DESCRIPTION = RunDescription('doom_basic_envs_appo', experiments=_experiments) Runner script should be importable (i.e. be in your project or in PYTHONPATH), and should define a single variable RUN_DESCRIPTION , which contains a list of experiments (each experiment can be a hyperparameter search), as well as some auxiliary parameters. When such a script is saved i.e. at myproject/train_10_seeds.py in your project using Sample Factory, you can use this command to execute it: python -m sample_factory.runner.run --run=myproject.train_10_seeds --runner=processes --max_parallel=12 --pause_between=10 --experiments_per_gpu=3 --num_gpus=4 This will cycle through the requested configurations, training 12 experiments at the same time, 3 per GPU on 4 GPUs using local OS-level parallelism. Runner supports other backends for parallel execution: --runner=slurm and --runner=ngc for Slurm and NGC support respectively. Individual experiments will be stored in train_dir/run_name so the whole experiment can be easily monitored with a single Tensorboard command. Find more information on runner API in runner/README.md .","title":"Runner interface"},{"location":"get-started/running-experiments/#dummy-sampler","text":"This tool can be useful if you want to estimate the upper bound on performance of any reinforcement learning algorithm, i.e. how fast the environment can be sampled by a dumb random policy. This achieves 90000+ FPS on a 10-core workstation: python -m sample_factory.run_algorithm --algo=DUMMY_SAMPLER --env=doom_benchmark --num_workers=20 --num_envs_per_worker=1 --experiment=dummy_sampler --sample_env_frames=5000000","title":"Dummy sampler"},{"location":"get-started/running-experiments/#tests","text":"To run unit tests execute ./all_tests.sh from the root of the repo. Consider installing VizDoom for a more comprehensive set of tests.","title":"Tests"},{"location":"get-started/running-experiments/#trained-policies","text":"See a separate trained_policies/README.md .","title":"Trained policies"},{"location":"get-started/running-experiments/#caveats","text":"Multiplayer VizDoom environments can freeze your console sometimes, simple reset takes care of this Sometimes VizDoom instances don't clear their internal shared memory buffers used to communicate between Python and a Doom executable. The file descriptors for these buffers tend to pile up. rm /dev/shm/ViZDoom* will take care of this issue. It's best to use the standard --fps=35 to visualize VizDoom results. --fps=0 enables Async execution mode for the Doom environments, although the results are not always reproducible between sync and async modes. Multiplayer VizDoom environments are significantly slower than single-player envs because actual network communication between the environment instances is required which results in a lot of syscalls. For prototyping and testing consider single-player environments with bots instead. Vectors of environments on rollout (actor) workers are instantiated on the same CPU thread. This can create problems for certain types of environment that require global per-thread or per-process context (e.g. OpenGL context). The solution should be an environment wrapper that starts the environment in a separate thread (or process if that's required) and communicates. doom_multiagent_wrapper.py is an example, although not optimal.","title":"Caveats"},{"location":"get-started/running-slurm/","text":"How to use SF2 on Slurm \u00b6 This doc contains instructions for running Sample-Factory v2 using slurm Setting up \u00b6 Login to your slurm login node using ssh with your username and password. Start an interactive job with srun to install files to your NFS. srun -c40 --gres=gpu:1 --pty bash Note that you may get a message groups: cannot find name for group ID XXXX Install Miniconda - Download installer using wget from https://docs.conda.io/en/latest/miniconda.html#linux-installers - Run the installer with bash {Miniconda...sh} Make new conda environment conda create --name sf2 then conda activate sf2 Download Sample-Factory and install dependencies for SF2 git clone https://github.com/alex-petrenko/sample-factory.git cd sample-factory git checkout sf2 pip install -e . Necessary scripts in SF2 \u00b6 To run a custom runner script for SF2 on slurm, you may need to write your own slurm_sbatch_template and/or runner script. slurm_sbatch_template is a bash script that run by slurm before your python script. It includes commands to activate your conda environment etc. See an example at ./sample_factory/runner/slurm/sbatch_template.sh . Variables in the bash script can be added in sample_factory.runner.run_slurm . The runner script controls the python command slurm will run. Examples are located in sf_examples . You can run multiple experiments with different parameters using ParamGrid . Timeout Batch Script \u00b6 If your slurm cluster has time limits for jobs, you can use the sbatch_timeout.sh bash script to launch jobs that timeout and requeue themselves before the time limit. The time limit can be set with the slurm_timeout command line argument. It defaults to 0 which runs the job with no time limit. It is recommended the timeout be set to slightly less than the time limit of your job. For example, if the time limit is 24 hours, you should set --slurm_timeout=23h Running runner scripts \u00b6 Return to the login node with exit Setup slurm output folder mkdir sf2 Activate your conda environment with bash and conda activate sf2 then cd sample-factory Run your runner script - an example mujuco runner (replace run, slurm_sbatch_template, and slurm_workdir with appropriate values) python -m sample_factory.runner.run --runner=slurm --slurm_workdir=./slurm_mujoco --experiment_suffix=slurm --slurm_gpus_per_job=1 --slurm_cpus_per_gpu=16 --slurm_sbatch_template=./sample_factory/runner/slurm/sbatch_template.sh --pause_between=1 --slurm_print_only=False --run=sf_examples.mujoco_examples.experiments.mujoco_all_envs The slurm_gpus_per_job and slurm_cpus_per_gpu determine the resources allocated to each job. You can view the jobs without running them by setting slurm_print_only=True . You can view the status of your jobs on nodes or the queue with squeue and view the outputs of your experiments with tail -f {slurm_workdir}/*.out . Cancel your jobs with scancel {job_id}","title":"How to use SF2 on Slurm"},{"location":"get-started/running-slurm/#how-to-use-sf2-on-slurm","text":"This doc contains instructions for running Sample-Factory v2 using slurm","title":"How to use SF2 on Slurm"},{"location":"get-started/running-slurm/#setting-up","text":"Login to your slurm login node using ssh with your username and password. Start an interactive job with srun to install files to your NFS. srun -c40 --gres=gpu:1 --pty bash Note that you may get a message groups: cannot find name for group ID XXXX Install Miniconda - Download installer using wget from https://docs.conda.io/en/latest/miniconda.html#linux-installers - Run the installer with bash {Miniconda...sh} Make new conda environment conda create --name sf2 then conda activate sf2 Download Sample-Factory and install dependencies for SF2 git clone https://github.com/alex-petrenko/sample-factory.git cd sample-factory git checkout sf2 pip install -e .","title":"Setting up"},{"location":"get-started/running-slurm/#necessary-scripts-in-sf2","text":"To run a custom runner script for SF2 on slurm, you may need to write your own slurm_sbatch_template and/or runner script. slurm_sbatch_template is a bash script that run by slurm before your python script. It includes commands to activate your conda environment etc. See an example at ./sample_factory/runner/slurm/sbatch_template.sh . Variables in the bash script can be added in sample_factory.runner.run_slurm . The runner script controls the python command slurm will run. Examples are located in sf_examples . You can run multiple experiments with different parameters using ParamGrid .","title":"Necessary scripts in SF2"},{"location":"get-started/running-slurm/#timeout-batch-script","text":"If your slurm cluster has time limits for jobs, you can use the sbatch_timeout.sh bash script to launch jobs that timeout and requeue themselves before the time limit. The time limit can be set with the slurm_timeout command line argument. It defaults to 0 which runs the job with no time limit. It is recommended the timeout be set to slightly less than the time limit of your job. For example, if the time limit is 24 hours, you should set --slurm_timeout=23h","title":"Timeout Batch Script"},{"location":"get-started/running-slurm/#running-runner-scripts","text":"Return to the login node with exit Setup slurm output folder mkdir sf2 Activate your conda environment with bash and conda activate sf2 then cd sample-factory Run your runner script - an example mujuco runner (replace run, slurm_sbatch_template, and slurm_workdir with appropriate values) python -m sample_factory.runner.run --runner=slurm --slurm_workdir=./slurm_mujoco --experiment_suffix=slurm --slurm_gpus_per_job=1 --slurm_cpus_per_gpu=16 --slurm_sbatch_template=./sample_factory/runner/slurm/sbatch_template.sh --pause_between=1 --slurm_print_only=False --run=sf_examples.mujoco_examples.experiments.mujoco_all_envs The slurm_gpus_per_job and slurm_cpus_per_gpu determine the resources allocated to each job. You can view the jobs without running them by setting slurm_print_only=True . You can view the status of your jobs on nodes or the queue with squeue and view the outputs of your experiments with tail -f {slurm_workdir}/*.out . Cancel your jobs with scancel {job_id}","title":"Running runner scripts"},{"location":"release-notes/release-notes/","text":"Recent releases \u00b6 v1.121.4 \u00b6 Support Weights and Biases (see section \"WandB support\") More configurable population-based training: can set from command line whether or not to mutate gamma, plus the perturbation magnitude for all float hyperparams can also be set from command line: --pbt_optimize_gamma: Whether to optimize gamma, discount factor, or not (experimental) (default: False) --pbt_perturb_min: When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.05) --pbt_perturb_max: When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.5) v1.121.3 \u00b6 Fixed a small bug related to population-based training (a reward shaping dictionary was assumed to be a flat dict, while it could be a nested dict in some envs) v1.121.2 \u00b6 Fixed a bug that prevented Vizdoom *.cfg and *.wad files from being copied to site-packages during installation from PyPI Added example on how to use custom Vizdoom envs without modifying the source code ( sample_factory_examples/train_custom_vizdoom_env.py ) v1.121.0 \u00b6 Added fixed KL divergence penalty as in https://arxiv.org/pdf/1707.06347.pdf Its usage is highly encouraged in environments with continuous action spaces (i.e. set --kl_loss_coeff=1.0). Otherwise numerical instabilities can occur in certain environments, especially when the policy lag is high More summaries related to the new loss v1.120.2 \u00b6 More improvements and fixes in runner interface, including support for NGC cluster v1.120.1 \u00b6 Runner interface improvements for Slurm v1.120.0 \u00b6 Support inactive agents. To deactivate an agent for a portion of the episode the environment should return info={'is_active': False} for the inactive agent. Useful for environments such as hide-n-seek. Improved memory consumption and performance with better shared memory management. Experiment logs are now saved into the experiment folder as sf_log.txt DMLab-related bug fixes (courtesy of @donghoonlee04 and @sungwoong . Thank you!)","title":"Recent releases"},{"location":"release-notes/release-notes/#recent-releases","text":"","title":"Recent releases"},{"location":"release-notes/release-notes/#v11214","text":"Support Weights and Biases (see section \"WandB support\") More configurable population-based training: can set from command line whether or not to mutate gamma, plus the perturbation magnitude for all float hyperparams can also be set from command line: --pbt_optimize_gamma: Whether to optimize gamma, discount factor, or not (experimental) (default: False) --pbt_perturb_min: When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.05) --pbt_perturb_max: When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.5)","title":"v1.121.4"},{"location":"release-notes/release-notes/#v11213","text":"Fixed a small bug related to population-based training (a reward shaping dictionary was assumed to be a flat dict, while it could be a nested dict in some envs)","title":"v1.121.3"},{"location":"release-notes/release-notes/#v11212","text":"Fixed a bug that prevented Vizdoom *.cfg and *.wad files from being copied to site-packages during installation from PyPI Added example on how to use custom Vizdoom envs without modifying the source code ( sample_factory_examples/train_custom_vizdoom_env.py )","title":"v1.121.2"},{"location":"release-notes/release-notes/#v11210","text":"Added fixed KL divergence penalty as in https://arxiv.org/pdf/1707.06347.pdf Its usage is highly encouraged in environments with continuous action spaces (i.e. set --kl_loss_coeff=1.0). Otherwise numerical instabilities can occur in certain environments, especially when the policy lag is high More summaries related to the new loss","title":"v1.121.0"},{"location":"release-notes/release-notes/#v11202","text":"More improvements and fixes in runner interface, including support for NGC cluster","title":"v1.120.2"},{"location":"release-notes/release-notes/#v11201","text":"Runner interface improvements for Slurm","title":"v1.120.1"},{"location":"release-notes/release-notes/#v11200","text":"Support inactive agents. To deactivate an agent for a portion of the episode the environment should return info={'is_active': False} for the inactive agent. Useful for environments such as hide-n-seek. Improved memory consumption and performance with better shared memory management. Experiment logs are now saved into the experiment folder as sf_log.txt DMLab-related bug fixes (courtesy of @donghoonlee04 and @sungwoong . Thank you!)","title":"v1.120.0"}]}